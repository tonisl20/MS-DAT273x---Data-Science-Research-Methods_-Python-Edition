{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 3, Lab 2 - Association\n",
    "=============================\n",
    "\n",
    "In this lab, we will examine how to analyze data for a correlation. Note\n",
    "that a detailed dive into correlational and regression-based research is\n",
    "given in Module 5. However, a brief overview is provided here. I focus\n",
    "on correlation because it is the simplest way to make an association\n",
    "claim, but as we saw in the online lesson, actually the correct analysis\n",
    "depends on your data (continuous, discrete, normal vs non-normal, etc.).\n",
    "Thus, a full illustration of all association techniques would take many,\n",
    "many labs. I focus on correlation here.\n",
    "\n",
    "In this example, you are analyzing customer loyalty data. Your\n",
    "organization uses three measures of loyalty, and you wish to test them\n",
    "out. (To avoid discussions of popular real measures, we will name these\n",
    "`loytalty1`, `loyalty2`, and `loyalty3`).\n",
    "\n",
    "Note that this lab uses the `ggplot2` package for data visualization and\n",
    "the `psych` package for correlation testing. I also assume you are\n",
    "familiar with `ggplot2`. As an alternative to the `psych` tools, we can\n",
    "also use the `Hmisc` package for correlation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD PACKAGES ####\n",
    "## Use inline magic command so plots appear in the data frame\n",
    "%matplotlib inline\n",
    "\n",
    "## Next the packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You load the data from the CSV file in the github folder for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "dat = pd.read_csv(\"datasets/loyaltydata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.columns)\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an ID variable shown as `Unnamed` and scores on a loyalty measures named\n",
    "`loyalty1` through `loyalty3`. \n",
    "\n",
    "****\n",
    "**Note:** You are not yet familiar with the scaling of these measures.\n",
    "***\n",
    "\n",
    "The first thing to do is to explore the variables. The Pandas `describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a sense as to the range and scaling of each loyalty\n",
    "measure.\n",
    "\n",
    "Imagine that each loyalty measure was in common use. You might want to\n",
    "know whether they are highly correlated. We can compute correlations\n",
    "between variables with the Pandas `cor()` method. A subset of the data frame is taken (using the outer `[]` operator) by providing a list (the inner `[]`) of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[['loyalty1', 'loyalty2', 'loyalty3']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difficult to read. Let's use the Pandas `round()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = dat[['loyalty1', 'loyalty2', 'loyalty3']].corr().round(2)\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the variables are *not* highly correlated with each\n",
    "other. This is a potential problem.\n",
    "\n",
    "A brief refresher: correlations range between zero (no association\n",
    "between variables) and 1.0 (a one-to-one association). They can also be\n",
    "positive (as one variable increases, so does the other) or negative (as\n",
    "one variable increases, the other decreases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistician Jacob Cohen suggested the following guidelines:  \n",
    "\n",
    "|       | Correlaton           | Meaning  |\n",
    "| ------------- |:-------------:|-----|\n",
    "| 1  | 0.0 - 0.1 | Negligible |\n",
    "|2   | 0.1 - 0.3      |   Small |\n",
    "| 3 | 0.3 - 0.5      |   Medium |\n",
    "| 4 |  0.5 +      |  Large |\n",
    "\n",
    "\n",
    "However, given that they are all ostensibly measuring the same thing,\n",
    "loyalty, we should expect much higher correlations (.7-.9).\n",
    "\n",
    "We can also easily visualize this correlation with using the `heatmap` function from the Python Seaborn package. Seaborn is a sophisticated statistical charting package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_mat, vmax=1.0) \n",
    "plt.title('Correlation matrix for loyalty features')\n",
    "plt.yticks(rotation='horizontal')\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will create scatter plots of each pairwise combination of loyalty variables. The code uses the `lmplot` function from Seaborn. Jitter on both the x and y axes along with high point transparency are used to help deal with over-plotting. Notice that the transparency argument, `alpha`, must be passed to the underling Matplotlib in a dictionary called `scatter_kws`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"loyalty1\", \"loyalty2\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"loyalty1\", \"loyalty3\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"loyalty2\", \"loyalty3\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-8-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-8-3.png)\n",
    "All of the graphs look about the same. It is always good to inspect the\n",
    "plots, as we know that non-linearity can weaken our correlations. Here,\n",
    "we see evidence that each measure is correlated linearly; the\n",
    "associations are simply underwhelming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-9-1.png)\n",
    "\n",
    "We can easily compute the confidence intervals of these correlation coefficients. However, this requires a few steps (don't worry if you don't follow this completely:   \n",
    "1. Transform the correlation from the initial space which we call r to a transformed space z. The distribution of errors is Normal in this transformed space. \n",
    "2. Compute the CI in the transformed space.\n",
    "3. Transform back to the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_z(r):\n",
    "    return math.log((1 + r) / (1 - r)) / 2.0\n",
    "\n",
    "def z_r(z):\n",
    "    e = math.exp(2 * z)\n",
    "    return((e - 1) / (e + 1))\n",
    "\n",
    "def r_conf_int(r, alpha, n):\n",
    "    # Transform r to z space\n",
    "    z = r_z(r)\n",
    "    # Compute standard error and critcal value in z\n",
    "    se = 1.0 / math.sqrt(n - 3)\n",
    "    z_crit = ss.norm.ppf(1 - alpha/2)\n",
    "\n",
    "    ## Compute CIs with transform to r\n",
    "    lo = z_r(z - z_crit * se)\n",
    "    hi = z_r(z + z_crit * se)\n",
    "    return (lo, hi)\n",
    "\n",
    "print('\\nFor loyalty1 vs. loyalty2')\n",
    "corr_mat = np.array(corr_mat)\n",
    "conf_ints = r_conf_int(corr_mat[1,0], 0.05, 1000)\n",
    "print('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[1,0], conf_ints[0], conf_ints[1]))\n",
    "print('\\nFor loyalty1 vs. loyalty3')\n",
    "conf_ints = r_conf_int(corr_mat[2,0], 0.05, 1000)\n",
    "print('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[2,0], conf_ints[0], conf_ints[1]))\n",
    "print('\\nFor loyalty2 vs. loyalty3')\n",
    "conf_ints = r_conf_int(corr_mat[2,1], 0.05, 1000)\n",
    "print('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[2,1], conf_ints[0], conf_ints[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the CIs of all the correlation coefficients are relatively small compared to the correlation coefficients. This indicates that these coefficients are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Claims Can We Make?\n",
    "========================\n",
    "\n",
    "Here, we can make the following claims: each of these variables is\n",
    "correlated with each other, but in reality, the correlations are weaker\n",
    "than you would hope them to be. In this case, we can have a series of\n",
    "conversations about whether these measures of loyalty are assessing\n",
    "different things, whether there are actually different kinds of customer\n",
    "loyalty, or whether the measures are not of high quality. Regardless,\n",
    "there appears to *not* be a large association between our measures of\n",
    "loyalty. In fact, using 95% CIs, we found that we had fairly precise\n",
    "estimate of our correlations: they are not strong. This raises large\n",
    "implications for our organization as it considers using these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
